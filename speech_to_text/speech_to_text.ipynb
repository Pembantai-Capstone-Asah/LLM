{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1O1vnU2fXJm"
      },
      "source": [
        "# Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hHUCFwN1M_Bs",
        "outputId": "7509e6b9-cd25-43bc-e6ad-dd2ff4edcaab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-o6hpcfl2\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-o6hpcfl2\n",
            "  Resolved https://github.com/openai/whisper.git to commit c0d2f624c09dc18e709e37c2ad90c039a4eb72a2\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (10.8.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (2.0.2)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (0.12.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (2.8.0+cu126)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (4.67.1)\n",
            "Requirement already satisfied: triton>=2 in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (3.4.0)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.12/dist-packages (from triton>=2->openai-whisper==20250625) (75.2.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba->openai-whisper==20250625) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken->openai-whisper==20250625) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken->openai-whisper==20250625) (2.32.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (4.15.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (1.11.1.6)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (2025.10.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->openai-whisper==20250625) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->openai-whisper==20250625) (3.0.3)\n",
            "Requirement already satisfied: jiwer in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.12/dist-packages (from jiwer) (8.3.0)\n",
            "Requirement already satisfied: rapidfuzz>=3.9.7 in /usr/local/lib/python3.12/dist-packages (from jiwer) (3.14.1)\n",
            "Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.12/dist-packages (2.5.5)\n",
            "Requirement already satisfied: torch>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from pytorch-lightning) (2.8.0+cu126)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.12/dist-packages (from pytorch-lightning) (4.67.1)\n",
            "Requirement already satisfied: PyYAML>5.4 in /usr/local/lib/python3.12/dist-packages (from pytorch-lightning) (6.0.3)\n",
            "Requirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (2025.3.0)\n",
            "Requirement already satisfied: torchmetrics>0.7.0 in /usr/local/lib/python3.12/dist-packages (from pytorch-lightning) (1.8.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from pytorch-lightning) (25.0)\n",
            "Requirement already satisfied: typing-extensions>4.5.0 in /usr/local/lib/python3.12/dist-packages (from pytorch-lightning) (4.15.0)\n",
            "Requirement already satisfied: lightning-utilities>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from pytorch-lightning) (0.15.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (3.13.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>=0.10.0->pytorch-lightning) (75.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.20.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.4.0)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics>0.7.0->pytorch-lightning) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.22.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.1.0->pytorch-lightning) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.1.0->pytorch-lightning) (3.0.3)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.12/dist-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (3.11)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.12/dist-packages (0.4.6)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (4.0.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.0.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.12/dist-packages (from evaluate) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from evaluate) (3.6.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.35.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from evaluate) (25.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (3.20.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (6.0.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.13.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.1.10)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2025.10.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
            "Requirement already satisfied: mutagen in /usr/local/lib/python3.12/dist-packages (1.47.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.35.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.10.5)\n"
          ]
        }
      ],
      "source": [
        "! pip install git+https://github.com/openai/whisper.git\n",
        "! pip install jiwer\n",
        "! pip install pytorch-lightning\n",
        "! pip install evaluate\n",
        "! pip install mutagen\n",
        "! pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GRrb-y6NOXpn"
      },
      "outputs": [],
      "source": [
        "import os, numpy as np, pathlib\n",
        "import torch, torch.nn\n",
        "import pandas as pd\n",
        "import whisper\n",
        "import torchaudio, torchaudio.transforms\n",
        "from pytorch_lightning import LightningModule, Trainer, seed_everything\n",
        "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "from tqdm.notebook import tqdm\n",
        "import matplotlib.pyplot as plt, seaborn as sns\n",
        "import evaluate\n",
        "from transformers.optimization import get_linear_schedule_with_warmup\n",
        "import jiwer\n",
        "from whisper.normalizers import EnglishTextNormalizer\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import mutagen\n",
        "from transformers import get_linear_schedule_with_warmup, AutoProcessor, AutoModelForSpeechSeq2Seq, pipeline\n",
        "from torch.optim import AdamW\n",
        "from pathlib import Path\n",
        "import openpyxl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jr7cLp9snVG"
      },
      "source": [
        "#### Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFULSo_DsnVH",
        "outputId": "b7541b11-d4f9-45fb-cacf-858cf670a5d7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:lightning_fabric.utilities.seed:Seed set to 3407\n"
          ]
        }
      ],
      "source": [
        "DATASET_DIR = \"dataset\"\n",
        "SAMPLE_RATE = 16000\n",
        "AUDIO_MAX_LENGTH = 160000  # Maksimal panjang audio (10 detik * 16000)\n",
        "TEXT_MAX_LENGTH = 200  # Maksimal panjang text transcript\n",
        "\n",
        "TRAIN_RATE = 0.8  # 80% untuk training\n",
        "VAL_RATE = 0.1    # 10% untuk validation\n",
        "TEST_RATE = 0.1   # 10% untuk testing\n",
        "\n",
        "TRAIN_BATCH_SIZE = 64  # Batch size untuk training\n",
        "EVAL_BATCH_SIZE = 16    # Batch size untuk evaluation\n",
        "MAX_TRAIN_STEPS = 150\n",
        "\n",
        "SEED = 3407\n",
        "seed_everything(SEED, workers=True)\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "class Config:\n",
        "\n",
        "    sample_rate = 16000\n",
        "\n",
        "    learning_rate = 1e-5\n",
        "    weight_decay = 0.01\n",
        "    adam_epsilon = 1e-8\n",
        "\n",
        "    warmup_steps = 10  # 10 steps warmup dari 150 total steps\n",
        "    max_steps = 150           # Minimum 150 Steps Training\n",
        "    eval_steps = 25           # Evaluate setiap 25 steps\n",
        "    save_steps = 50           # Save checkpoint setiap 50 steps\n",
        "    train_batch_size = 8      # Batch size training\n",
        "    eval_batch_size = 4       # Batch size evaluation\n",
        "    num_worker = 2            # Number of workers untuk data loading\n",
        "    gradient_accumulation_steps = 2  # Accumulate gradients untuk simulate larger batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_6UDjkDsnVH"
      },
      "source": [
        "#### Utility Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpGLLrRgO4QV"
      },
      "outputs": [],
      "source": [
        "def load_wave(wave_path, sample_rate: int = 16000):\n",
        "    waveform = whisper.load_audio(wave_path, sr=sample_rate)\n",
        "    return torch.from_numpy(waveform)\n",
        "\n",
        "\n",
        "def calculate_err(data):\n",
        "    normalizer = EnglishTextNormalizer()\n",
        "\n",
        "    data['text_clean'] = data['text'].apply(normalizer)\n",
        "    data['predict_clean'] = data['predict'].apply(normalizer)\n",
        "\n",
        "    wer_original = jiwer.wer(list(data['text']), list(data['predict']))\n",
        "    cer_original = jiwer.cer(list(data['text']), list(data['predict']))\n",
        "\n",
        "    wer_normalized = jiwer.wer(list(data['text_clean']), list(data['predict_clean']))\n",
        "    cer_normalized = jiwer.cer(list(data['text_clean']), list(data['predict_clean']))\n",
        "\n",
        "    return (data, wer_original, wer_normalized, cer_original, cer_normalized)\n",
        "\n",
        "\n",
        "def extract_aud(audio_path, mymodel):\n",
        "    result = mymodel.transcribe(audio_path, language=\"id\", without_timestamps=True)\n",
        "    return result[\"text\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOUHOT0lsnVH"
      },
      "source": [
        "#### Dataset Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zUWLt0uusnVH"
      },
      "outputs": [],
      "source": [
        "TRANSCRIPTION_DIR = \"korpus_tvri.csv\"\n",
        "\n",
        "def load_custom_dataset():\n",
        "    audio_transcript_pair_list = []\n",
        "\n",
        "    df = pd.read_csv(TRANSCRIPTION_DIR)\n",
        "\n",
        "    for i in range(len(df)):\n",
        "        audio_path = os.path.join(DATASET_DIR, f\"{df.loc[i, 'Nama Data']}.wav\")\n",
        "        text = df.loc[i, \"Transkrip Suara (Bahasa Indonesia)\"]\n",
        "\n",
        "        print(f\"{audio_path}\")\n",
        "        audio = mutagen.File(audio_path)\n",
        "        audio_length = audio.info.length * 16000\n",
        "        if len(text) <= TEXT_MAX_LENGTH and audio_length <= AUDIO_MAX_LENGTH:\n",
        "            audio_transcript_pair_list.append((audio_path, text))\n",
        "\n",
        "    return audio_transcript_pair_list\n",
        "\n",
        "\n",
        "def split_dataset(audio_transcript_pair_list, train_rate=0.8, val_rate=0.1, test_rate=0.1):\n",
        "    np.random.seed(SEED)\n",
        "    np.random.shuffle(audio_transcript_pair_list)\n",
        "\n",
        "    total_rate = train_rate + val_rate + test_rate\n",
        "    if abs(total_rate - 1.0) > 1e-9:\n",
        "        print(f\"Warning: Dataset split rates ({total_rate}) do not sum to 1.0. Adjusting.\")\n",
        "        train_rate /= total_rate\n",
        "        val_rate /= total_rate\n",
        "        test_rate /= total_rate\n",
        "\n",
        "    dataset_size = len(audio_transcript_pair_list)\n",
        "\n",
        "    train_size = int(train_rate * dataset_size)\n",
        "    val_size = int(val_rate * dataset_size)\n",
        "    test_size = dataset_size - train_size - val_size\n",
        "\n",
        "    print(f\"Train size: {train_size}, Val size: {val_size}, Test size: {test_size}\")\n",
        "\n",
        "    train_list = audio_transcript_pair_list[:train_size]\n",
        "    val_list = audio_transcript_pair_list[train_size:train_size + val_size]\n",
        "    test_list = audio_transcript_pair_list[train_size + val_size:]\n",
        "\n",
        "    return train_list, val_list, test_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYkh9CKLsnVI"
      },
      "source": [
        "#### Dataset Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3qrQ6hg3snVI"
      },
      "outputs": [],
      "source": [
        "class CustomSpeechDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, audio_info_list, tokenizer, sample_rate):\n",
        "        super().__init__()\n",
        "\n",
        "        self.audio_info_list = audio_info_list\n",
        "        self.tokenizer = tokenizer\n",
        "        self.sample_rate = sample_rate\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.audio_info_list)\n",
        "\n",
        "    def __getitem__(self, id):\n",
        "        audio_path, text = self.audio_info_list[id]\n",
        "\n",
        "        waveform = load_wave(audio_path, sample_rate=self.sample_rate)\n",
        "        waveform = whisper.pad_or_trim(waveform.flatten())\n",
        "        mel = whisper.log_mel_spectrogram(waveform)\n",
        "\n",
        "        text_tokens = self.tokenizer.encode(text)\n",
        "        text = [*self.tokenizer.sot_sequence_including_notimestamps] + self.tokenizer.encode(text)\n",
        "        labels = text[1:] + [self.tokenizer.eot]\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": mel,\n",
        "            \"labels\": labels,\n",
        "            \"dec_input_ids\": text\n",
        "        }\n",
        "\n",
        "class WhisperDataCollatorWithPadding:\n",
        "\n",
        "    def __call__(self, features):\n",
        "\n",
        "        input_ids, labels, dec_input_ids = [], [], []\n",
        "        for f in features:\n",
        "            input_ids.append(f[\"input_ids\"])\n",
        "            labels.append(f[\"labels\"])\n",
        "            dec_input_ids.append(f[\"dec_input_ids\"])\n",
        "\n",
        "        input_ids = torch.concat([input_id[None, :] for input_id in input_ids])\n",
        "\n",
        "        label_lengths = [len(lab) for lab in labels]\n",
        "        dec_input_ids_length = [len(e) for e in dec_input_ids]\n",
        "        max_label_len = max(label_lengths+dec_input_ids_length)\n",
        "\n",
        "        labels = [np.pad(lab, (0, max_label_len - lab_len), 'constant', constant_values=-100) for lab, lab_len in zip(labels, label_lengths)]\n",
        "        dec_input_ids = [np.pad(e, (0, max_label_len - e_len), 'constant', constant_values=50257) for e, e_len in zip(dec_input_ids, dec_input_ids_length)] # 50257 is eot token id\n",
        "\n",
        "        batch = {\n",
        "            \"labels\": labels,\n",
        "            \"dec_input_ids\": dec_input_ids\n",
        "        }\n",
        "\n",
        "        batch = {k: torch.tensor(np.array(v), requires_grad=False) for k, v in batch.items()}\n",
        "        batch[\"input_ids\"] = input_ids\n",
        "\n",
        "        return batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uB20Br8_uSYn"
      },
      "source": [
        "#### Whisper Finetuning Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n0VEm-5iuYDd"
      },
      "outputs": [],
      "source": [
        "# class WhisperModelModule(LightningModule):\n",
        "\n",
        "#     def __init__(self, cfg, model_name=\"tiny\", lang=\"id\", train_dataset=[], eval_dataset=[]):\n",
        "#         super().__init__()\n",
        "#         self.options = whisper.DecodingOptions(language=\"id\", without_timestamps=True, task=\"transcribe\")\n",
        "#         self.tokenizer = whisper.tokenizer.get_tokenizer(True, language=\"id\", task=self.options.task)\n",
        "\n",
        "#         self.model = whisper.load_model(model_name)\n",
        "\n",
        "#         self.loss_fn = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
        "#         self.metrics_wer = evaluate.load(\"wer\")\n",
        "#         self.metrics_cer = evaluate.load(\"cer\")\n",
        "\n",
        "#         self.cfg = cfg\n",
        "#         self.train_dataset = train_dataset\n",
        "#         self.eval_dataset = eval_dataset\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         return self.model(x)\n",
        "\n",
        "#     def training_step(self, batch, batch_id):\n",
        "#         input_ids = batch[\"input_ids\"]\n",
        "#         labels = batch[\"labels\"].long()\n",
        "#         dec_input_ids = batch[\"dec_input_ids\"].long()\n",
        "\n",
        "#         with torch.no_grad():\n",
        "#             audio_features = self.model.encoder(input_ids)\n",
        "\n",
        "#         out = self.model.decoder(dec_input_ids, audio_features)\n",
        "\n",
        "#         loss = self.loss_fn(out.view(-1, out.size(-1)), labels.view(-1))\n",
        "\n",
        "#         self.log(\"train/loss\", loss, on_step=True, prog_bar=True, logger=True)\n",
        "\n",
        "#         return loss\n",
        "\n",
        "#     def validation_step(self, batch, batch_id):\n",
        "#         input_ids = batch[\"input_ids\"]\n",
        "#         labels = batch[\"labels\"].long()\n",
        "#         dec_input_ids = batch[\"dec_input_ids\"].long()\n",
        "#         audio_features = self.model.encoder(input_ids)\n",
        "#         out = self.model.decoder(dec_input_ids, audio_features)\n",
        "\n",
        "#         loss = self.loss_fn(out.view(-1, out.size(-1)), labels.view(-1))\n",
        "\n",
        "#         predicted_ids = torch.argmax(out, dim=2)\n",
        "\n",
        "#         o_list, l_list = [], []\n",
        "#         for pred, ref in zip(predicted_ids, labels):\n",
        "#             o_list.append(self.tokenizer.decode([token.item() for token in pred if token.item() != -100 and token.item() != self.tokenizer.eot]))\n",
        "#             l_list.append(self.tokenizer.decode([token.item() for token in ref if token.item() != -100]))\n",
        "\n",
        "#         cer = self.metrics_cer.compute(references=l_list, predictions=o_list)\n",
        "#         wer = self.metrics_wer.compute(references=l_list, predictions=o_list)\n",
        "\n",
        "\n",
        "#         self.log(\"val/loss\", loss, on_step=True, prog_bar=True, logger=True)\n",
        "#         self.log(\"val/cer\", cer, on_step=True, prog_bar=True, logger=True)\n",
        "#         self.log(\"val/wer\", wer, on_step=True, prog_bar=True, logger=True)\n",
        "\n",
        "#         return {\n",
        "#             \"cer\": cer,\n",
        "#             \"wer\": wer,\n",
        "#             \"loss\": loss\n",
        "#         }\n",
        "\n",
        "\n",
        "#     def configure_optimizers(self):\n",
        "\n",
        "#         model = self.model\n",
        "#         no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "#         optimizer_grouped_parameters = [\n",
        "#             {\n",
        "#                 \"params\": [p for n, p in model.named_parameters()\n",
        "#                             if not any(nd in n for nd in no_decay)],\n",
        "#                 \"weight_decay\": self.cfg.weight_decay,\n",
        "#             },\n",
        "#             {\n",
        "#                 \"params\": [p for n, p in model.named_parameters()\n",
        "#                             if any(nd in n for nd in no_decay)],\n",
        "#                 \"weight_decay\": 0.0,\n",
        "#             },\n",
        "#         ]\n",
        "#         optimizer = AdamW(optimizer_grouped_parameters,\n",
        "#                           lr=self.cfg.learning_rate,\n",
        "#                           eps=self.cfg.adam_epsilon)\n",
        "\n",
        "#         scheduler = get_linear_schedule_with_warmup(\n",
        "#             optimizer, num_warmup_steps=self.cfg.warmup_steps,\n",
        "#             num_training_steps=self.cfg.max_steps\n",
        "#         )\n",
        "\n",
        "#         return [optimizer], [{\"scheduler\": scheduler, \"interval\": \"step\", \"frequency\": 1}]\n",
        "\n",
        "#     def train_dataloader(self):\n",
        "#         dataset = CustomSpeechDataset(self.train_dataset, self.tokenizer, self.cfg.sample_rate)\n",
        "#         return torch.utils.data.DataLoader(dataset,\n",
        "#                           batch_size=self.cfg.train_batch_size,\n",
        "#                           drop_last=True, shuffle=True, num_workers=self.cfg.num_worker,\n",
        "#                           collate_fn=WhisperDataCollatorWithPadding()\n",
        "#                           )\n",
        "\n",
        "#     def val_dataloader(self):\n",
        "#         dataset = CustomSpeechDataset(self.eval_dataset, self.tokenizer, self.cfg.sample_rate)\n",
        "#         return torch.utils.data.DataLoader(dataset,\n",
        "#                           batch_size=self.cfg.eval_batch_size,\n",
        "#                           num_workers=self.cfg.num_worker,\n",
        "#                           collate_fn=WhisperDataCollatorWithPadding()\n",
        "#                           )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZGHL0-csnVI"
      },
      "source": [
        "#### Whisper From Scratch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EIAgnNoqsnVJ"
      },
      "outputs": [],
      "source": [
        "# class WhisperFromScratch:\n",
        "\n",
        "#     def __init__(self, model_name='tiny'):\n",
        "#         self.model = whisper.load_model(model_name)\n",
        "#         self._reset_parameters()\n",
        "#         self.options = whisper.DecodingOptions(language=\"id\", without_timestamps=True, task=\"transcribe\")\n",
        "#         self.tokenizer = whisper.tokenizer.get_tokenizer(True, language=\"id\", task=self.options.task)\n",
        "\n",
        "#     def _reset_parameters(self):\n",
        "#         for m in self.model.modules():\n",
        "#             if isinstance(m, torch.nn.Linear):\n",
        "#                 torch.nn.init.xavier_uniform_(m.weight)\n",
        "#                 if m.bias is not None:\n",
        "#                     torch.nn.init.constant_(m.bias, 0)\n",
        "#             elif isinstance(m, torch.nn.Conv1d):\n",
        "#                  torch.nn.init.xavier_uniform_(m.weight)\n",
        "#                  if m.bias is not None:\n",
        "#                     torch.nn.init.constant_(m.bias, 0)\n",
        "#             elif isinstance(m, torch.nn.ConvTranspose1d):\n",
        "#                  torch.nn.init.xavier_uniform_(m.weight)\n",
        "#                  if m.bias is not None:\n",
        "#                     torch.nn.init.constant_(m.bias, 0)\n",
        "#             elif isinstance(m, torch.nn.LayerNorm):\n",
        "#                 torch.nn.init.constant_(m.weight, 1)\n",
        "#                 torch.nn.init.constant_(m.bias, 0)\n",
        "\n",
        "\n",
        "#     def forward(self, mel, tokens):\n",
        "#         audio_features = self.model.encoder(mel)\n",
        "#         logits = self.model.decoder(tokens, audio_features)\n",
        "#         return logits\n",
        "\n",
        "#     def compute_loss(self, mel, labels):\n",
        "#         labels = labels.to(self.model.device)\n",
        "\n",
        "#         dec_input_ids = labels.clone()\n",
        "#         dec_input_ids[dec_input_ids == -100] = self.tokenizer.sot_sequence_including_notimestamps[0]\n",
        "\n",
        "#         max_token_id = self.model.decoder.token_embedding.num_embeddings - 1\n",
        "#         dec_input_ids = torch.clamp(dec_input_ids, max=max_token_id)\n",
        "\n",
        "#         with torch.no_grad():\n",
        "#           audio_features = self.model.encoder(mel.to(self.model.device))\n",
        "\n",
        "#         logits = self.model.decoder(dec_input_ids, audio_features)\n",
        "\n",
        "#         loss = torch.nn.functional.cross_entropy(logits.view(-1, logits.size(-1)), labels.view(-1), ignore_index=-100) # Use -100 for padding\n",
        "\n",
        "#         return loss\n",
        "\n",
        "#     def train_from_scratch(self, train_loader, val_loader, max_steps=150, learning_rate=1e-4, device='cuda'):\n",
        "#         optimizer = AdamW(self.model.parameters(), lr=learning_rate)\n",
        "#         scheduler = get_linear_schedule_with_warmup(\n",
        "#             optimizer, num_warmup_steps=1, num_training_steps=max_steps\n",
        "#         )\n",
        "\n",
        "#         self.model.to(device)\n",
        "#         self.model.train()\n",
        "\n",
        "#         train_losses = []\n",
        "#         val_losses = []\n",
        "#         best_val_loss = float('inf')\n",
        "\n",
        "#         train_iter = iter(train_loader)\n",
        "\n",
        "#         for step in tqdm(range(max_steps)):\n",
        "#             try:\n",
        "#                 batch = next(train_iter)\n",
        "#             except StopIteration:\n",
        "#                 train_iter = iter(train_loader)\n",
        "#                 batch = next(train_iter)\n",
        "\n",
        "#             mel = batch[\"input_ids\"].to(device)\n",
        "#             labels = batch[\"labels\"].to(device) # Labels are the targets\n",
        "\n",
        "#             loss = self.compute_loss(mel, labels)\n",
        "\n",
        "#             loss.backward()\n",
        "#             torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0) # Gradient clipping\n",
        "#             optimizer.step()\n",
        "#             scheduler.step()\n",
        "#             optimizer.zero_grad()\n",
        "\n",
        "#             train_losses.append(loss.item())\n",
        "\n",
        "#             if (step + 1) % Config().eval_steps == 0: # Evaluate every 25 steps\n",
        "#                 self.model.eval()\n",
        "#                 val_loss = 0\n",
        "#                 with torch.no_grad():\n",
        "#                     for val_batch in val_loader:\n",
        "#                         val_mel = val_batch[\"input_ids\"].to(device)\n",
        "#                         val_labels = val_batch[\"labels\"].to(device) # Labels are the targets\n",
        "#                         val_loss += self.compute_loss(val_mel, val_labels).item()\n",
        "#                 val_loss /= len(val_loader)\n",
        "#                 val_losses.append(val_loss)\n",
        "#                 print(f\"Step {step+1}: Train Loss = {train_losses[-1]:.4f}, Val Loss = {val_loss:.4f}\")\n",
        "\n",
        "#                 if val_loss < best_val_loss:\n",
        "#                     best_val_loss = val_loss\n",
        "#                     torch.save(self.model.state_dict(), \"best_scratch_model_indonesia.pth\")\n",
        "\n",
        "#                 self.model.train()\n",
        "\n",
        "#         return train_losses, val_losses\n",
        "\n",
        "\n",
        "#     def transcribe(self, audio_path):\n",
        "#         self.model.eval()\n",
        "#         with torch.no_grad():\n",
        "#             result = self.model.transcribe(audio_path, language=\"id\", without_timestamps=True)\n",
        "#         return result[\"text\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYdYF5oM2iuN"
      },
      "source": [
        "#### Split Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Z1k5JseHefU",
        "outputId": "37f4d405-5b45-452e-eea3-d3d751cd32d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dataset/TVRI_BS_071119_0001.wav\n",
            "dataset/TVRI_BS_071119_0002.wav\n",
            "dataset/TVRI_BS_071119_0003.wav\n",
            "dataset/TVRI_BS_071119_0004.wav\n",
            "dataset/TVRI_BS_071119_0005.wav\n",
            "dataset/TVRI_BS_071119_0006.wav\n",
            "dataset/TVRI_BS_071119_0007.wav\n",
            "dataset/TVRI_BS_071119_0010.wav\n",
            "dataset/TVRI_BS_071119_0013.wav\n",
            "dataset/TVRI_BS_071119_0019.wav\n",
            "dataset/TVRI_BS_071119_0020.wav\n",
            "dataset/TVRI_BS_071119_0022.wav\n",
            "Train size: 0, Val size: 0, Test size: 9\n",
            "Train samples: 0\n",
            "Val samples: 0\n",
            "Test samples: 9\n",
            "Is total samples match dataset: True\n"
          ]
        }
      ],
      "source": [
        "audio_transcript_pair_list = load_custom_dataset()\n",
        "train_list, val_list, test_list = split_dataset(audio_transcript_pair_list, 0, 0, 1)\n",
        "total_samples = len(train_list) + len(val_list) + len(test_list)\n",
        "print(f\"Train samples: {len(train_list)}\")\n",
        "print(f\"Val samples: {len(val_list)}\")\n",
        "print(f\"Test samples: {len(test_list)}\")\n",
        "print(f\"Is total samples match dataset: {total_samples == len(audio_transcript_pair_list)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFdGupONsnVJ"
      },
      "source": [
        "#### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P_A0O90g1gv3"
      },
      "outputs": [],
      "source": [
        "# log_output_dir = \"logs\"\n",
        "# check_output_dir = \"artifacts\"\n",
        "# train_name = \"whisper\"\n",
        "# model_name = \"medium\"\n",
        "# lang = \"id\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IthEpsWi03N9"
      },
      "outputs": [],
      "source": [
        "# cfg = Config()\n",
        "\n",
        "# Path(log_output_dir).mkdir(exist_ok=True)\n",
        "# Path(check_output_dir).mkdir(exist_ok=True)\n",
        "\n",
        "# tflogger = TensorBoardLogger(\n",
        "#     save_dir=log_output_dir,\n",
        "#     name=train_name,\n",
        "# )\n",
        "\n",
        "# checkpoint_callback = ModelCheckpoint(\n",
        "#     dirpath=f\"{check_output_dir}/checkpoint\",\n",
        "#     filename=\"checkpoint-{epoch:04d}\",\n",
        "#     save_top_k=-1 # all model save\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dWfVg9j200KY"
      },
      "outputs": [],
      "source": [
        "# options = whisper.DecodingOptions(language=\"id\", without_timestamps=True, task=\"transcribe\")\n",
        "# tokenizer = whisper.tokenizer.get_tokenizer(True, language=\"id\", task=options.task)\n",
        "\n",
        "# train_dataset_scratch = CustomSpeechDataset(train_list, tokenizer, SAMPLE_RATE)\n",
        "# val_dataset_scratch = CustomSpeechDataset(val_list, tokenizer, SAMPLE_RATE)\n",
        "\n",
        "# data_collator_scratch = WhisperDataCollatorWithPadding()\n",
        "\n",
        "# train_loader_scratch = torch.utils.data.DataLoader(\n",
        "#     train_dataset_scratch,\n",
        "#     batch_size=Config().train_batch_size,\n",
        "#     shuffle=True,\n",
        "#     collate_fn=data_collator_scratch\n",
        "# )\n",
        "\n",
        "# val_loader_scratch = torch.utils.data.DataLoader(\n",
        "#     val_dataset_scratch,\n",
        "#     batch_size=Config().eval_batch_size,\n",
        "#     shuffle=False,\n",
        "#     collate_fn=data_collator_scratch\n",
        "# )\n",
        "\n",
        "# print(\"STARTING FROM-SCRATCH TRAINING...\")\n",
        "\n",
        "# model_scratch = WhisperFromScratch('tiny')\n",
        "# train_losses_scratch, val_losses_scratch = model_scratch.train_from_scratch(\n",
        "#     train_loader_scratch,\n",
        "#     val_loader_scratch,\n",
        "#     max_steps=Config().max_steps,\n",
        "#     learning_rate=Config().learning_rate,\n",
        "#     device=DEVICE\n",
        "# )\n",
        "\n",
        "# print(\"FROM-SCRATCH TRAINING FINISHED.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BMF9O97Wegni"
      },
      "outputs": [],
      "source": [
        "# checkpoint_path = \"artifacts/checkpoint/checkpoint-epoch=0005.ckpt\"\n",
        "# state_dict = torch.load(checkpoint_path)\n",
        "# state_dict = state_dict['state_dict']\n",
        "\n",
        "# whisper_fine_tuned_model = WhisperModelModule(cfg)\n",
        "# whisper_fine_tuned_model.load_state_dict(state_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rprwI3hZfNSH"
      },
      "outputs": [],
      "source": [
        "# options = whisper.DecodingOptions(language=\"id\", without_timestamps=True, task=\"transcribe\")\n",
        "# tokenizer = whisper.tokenizer.get_tokenizer(True, language=\"id\", task=options.task)\n",
        "# dataset = CustomSpeechDataset(val_list, tokenizer, SAMPLE_RATE)\n",
        "# loader = torch.utils.data.DataLoader(dataset, batch_size=2, collate_fn=WhisperDataCollatorWithPadding())\n",
        "\n",
        "# refs = []\n",
        "# res = []\n",
        "# for b in tqdm(loader):\n",
        "#     input_ids = b[\"input_ids\"].half()\n",
        "#     labels = b[\"labels\"].long()\n",
        "#     with torch.no_grad():\n",
        "#         results = whisper_fine_tuned_model.model.decode(input_ids, options)\n",
        "#         for r in results:\n",
        "#             res.append(r.text)\n",
        "\n",
        "#         for l in labels:\n",
        "#             filtered_l = [token.item() for token in l if token.item() != -100 and token.item() != tokenizer.eot]\n",
        "#             ref = tokenizer.decode(filtered_l)\n",
        "#             refs.append(ref)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FIWl3h5NP5rk"
      },
      "outputs": [],
      "source": [
        "# cer_metrics = evaluate.load(\"cer\")\n",
        "# cer_metrics.compute(references=refs, predictions=res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AAcs4tWtP9xq"
      },
      "outputs": [],
      "source": [
        "# model_path = 'best_scratch_model_indonesia.pth'\n",
        "# state_dict = torch.load(model_path)\n",
        "# whisper_from_scratch_model = WhisperFromScratch('tiny')\n",
        "# whisper_from_scratch_model.model.load_state_dict(state_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T8kpttrHQ8Hi"
      },
      "outputs": [],
      "source": [
        "# options = whisper.DecodingOptions(language=\"id\", without_timestamps=True, task=\"transcribe\")\n",
        "# tokenizer = whisper.tokenizer.get_tokenizer(True, language=\"id\", task=options.task)\n",
        "# dataset = CustomSpeechDataset(val_list, tokenizer, SAMPLE_RATE)\n",
        "# loader = torch.utils.data.DataLoader(dataset, batch_size=2, collate_fn=WhisperDataCollatorWithPadding())\n",
        "\n",
        "# refs = []\n",
        "# res = []\n",
        "# for b in tqdm(loader):\n",
        "#     input_ids = b[\"input_ids\"].half()\n",
        "#     labels = b[\"labels\"].long()\n",
        "#     with torch.no_grad():\n",
        "#         results = whisper_from_scratch_model.model.decode(input_ids, options)\n",
        "#         for r in results:\n",
        "#             res.append(r.text)\n",
        "\n",
        "#         for l in labels:\n",
        "#             filtered_l = [token.item() for token in l if token.item() != -100 and token.item() != tokenizer.eot]\n",
        "#             ref = tokenizer.decode(filtered_l)\n",
        "#             refs.append(ref)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dqOGFcBkRzDu"
      },
      "outputs": [],
      "source": [
        "# cer_metrics = evaluate.load(\"cer\")\n",
        "# cer_metrics.compute(references=refs, predictions=res)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HdxIziysnVJ"
      },
      "source": [
        "#### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 473,
          "referenced_widgets": [
            "7196a5b5857e4c928d1efcffd259350c",
            "8e8e055ebd45452789c7b87a5d25fb1c",
            "c0a5ae1b303f4b1aa3d6838397284622",
            "e9c6b87fc5dd4b669dccb47207ea5855",
            "e2b5bb9426a542478f22c13053a3ea9a",
            "37f7f201af7c4da2a192cc33270c8b41",
            "88cfe2d442d1425c9ce0af3bd0b15c37",
            "dc81dd135d8f4d00bc0ca2375180abc8",
            "dea60e6019c445d6bc1b5a0a9cbf26bf",
            "ba1f48cd5fc349f08b983a1b96c300fe",
            "a966a961f12c45359f7589dedc761855"
          ]
        },
        "id": "IZiA5ITXlJiv",
        "outputId": "f4314a24-c84a-436f-8ed9-9d4e30666c1a"
      },
      "outputs": [],
      "source": [
        "test_df = pd.DataFrame(test_list, columns=['audio_path', 'text'])\n",
        "\n",
        "# EVALUATION 1: ZERO-SHOT (BASELINE)\n",
        "\n",
        "print(\"EVALUATING ZERO-SHOT MODEL...\")\n",
        "\n",
        "zero_shot_predictions = []\n",
        "for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Transcribing Zero-Shot\"):\n",
        "    print(row['audio_path'])\n",
        "\n",
        "    transcriber = pipeline(\n",
        "      \"automatic-speech-recognition\",\n",
        "      model=\"cahya/whisper-medium-id\"\n",
        "    )\n",
        "    transcriber.model.config.forced_decoder_ids = (\n",
        "      transcriber.tokenizer.get_decoder_prompt_ids(\n",
        "        language=\"id\",\n",
        "        task=\"transcribe\"\n",
        "      )\n",
        "    )\n",
        "    prediction = transcriber(row['audio_path'])[\"text\"]\n",
        "    zero_shot_predictions.append(prediction)\n",
        "\n",
        "test_df['predict_zero_shot'] = zero_shot_predictions\n",
        "_, wer_zero_shot_orig, wer_zero_shot_norm, cer_zero_shot_orig, cer_zero_shot_norm = calculate_err(test_df.rename(columns={'predict_zero_shot': 'predict'}))\n",
        "\n",
        "print(f\"Zero-Shot WER (Original): {wer_zero_shot_orig:.4f}\")\n",
        "print(f\"Zero-Shot WER (Normalized): {wer_zero_shot_norm:.4f}\")\n",
        "print(f\"Zero-Shot CER (Original): {cer_zero_shot_orig:.4f}\")\n",
        "print(f\"Zero-Shot CER (Normalized): {cer_zero_shot_norm:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "D-SRafCY5CUk",
        "outputId": "40e051dd-d9fe-46d3-dcf8-092e49067482"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"prediction_df[[\\\"text_clean\\\", \\\"predict_clean\\\"]]\",\n  \"rows\": 9,\n  \"fields\": [\n    {\n      \"column\": \"text_clean\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 9,\n        \"samples\": [\n          \"selamat malam anda menyaksikan indonesia hari ini kamis 7 november 2019 bersama saya happy goeritman\",\n          \"dan saudara berikut indonesia hari ini selengkapnya\",\n          \"segera akan dilakukan penahanan dan berkas perkara segera dirimpahkan ke jaksa penuntut umum\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"predict_clean\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 9,\n        \"samples\": [\n          \"selamat malam anda menyaksikan indonesia hari ini kamis 7 november 2019 bersama saya happy guritman\",\n          \"dan saudara berikut indonesia hari ini selengkapnya\",\n          \"segera akan dilakukan penahanan dan berkas perkara segera dilimpahkan ke jaksa penuntut umum\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-21d07c2b-e1ab-43e7-befe-e86c605848ed\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text_clean</th>\n",
              "      <th>predict_clean</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>dari hasil investigasi tersebut penyidik menet...</td>\n",
              "      <td>dari hasil investigasi tersebut penyidik menet...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>dan saudara berikut indonesia hari ini selengk...</td>\n",
              "      <td>dan saudara berikut indonesia hari ini selengk...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>yakni brigadir am dalam kasus kematian mahasis...</td>\n",
              "      <td>yakni bergadir am dalam kasus kematian mahasis...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>menteri pendidikan dan kebudayaan nadiem makar...</td>\n",
              "      <td>menteri pendidikan dan kemudayaan nadima karim...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>hasil investigasi penyidik bareskrim polri men...</td>\n",
              "      <td>hasil investigasi penyidik baris krim polri me...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>segera akan dilakukan penahanan dan berkas per...</td>\n",
              "      <td>segera akan dilakukan penahanan dan berkas per...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>dengan cara ketik pga spasi 7 c dan juga dukun...</td>\n",
              "      <td>dengan cara ketik pga spasi tujuh c dan juga d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>selamat malam anda menyaksikan indonesia hari ...</td>\n",
              "      <td>selamat malam anda menyaksikan indonesia hari ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>dan saya ardianto wijaya selain dua topik utam...</td>\n",
              "      <td>dan saya adyanto vijaya selain dua topik utama...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-21d07c2b-e1ab-43e7-befe-e86c605848ed')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-21d07c2b-e1ab-43e7-befe-e86c605848ed button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-21d07c2b-e1ab-43e7-befe-e86c605848ed');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-ca50ccff-01b4-41f5-8aef-239fe4430598\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ca50ccff-01b4-41f5-8aef-239fe4430598')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-ca50ccff-01b4-41f5-8aef-239fe4430598 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                          text_clean  \\\n",
              "0  dari hasil investigasi tersebut penyidik menet...   \n",
              "1  dan saudara berikut indonesia hari ini selengk...   \n",
              "2  yakni brigadir am dalam kasus kematian mahasis...   \n",
              "3  menteri pendidikan dan kebudayaan nadiem makar...   \n",
              "4  hasil investigasi penyidik bareskrim polri men...   \n",
              "5  segera akan dilakukan penahanan dan berkas per...   \n",
              "6  dengan cara ketik pga spasi 7 c dan juga dukun...   \n",
              "7  selamat malam anda menyaksikan indonesia hari ...   \n",
              "8  dan saya ardianto wijaya selain dua topik utam...   \n",
              "\n",
              "                                       predict_clean  \n",
              "0  dari hasil investigasi tersebut penyidik menet...  \n",
              "1  dan saudara berikut indonesia hari ini selengk...  \n",
              "2  yakni bergadir am dalam kasus kematian mahasis...  \n",
              "3  menteri pendidikan dan kemudayaan nadima karim...  \n",
              "4  hasil investigasi penyidik baris krim polri me...  \n",
              "5  segera akan dilakukan penahanan dan berkas per...  \n",
              "6  dengan cara ketik pga spasi tujuh c dan juga d...  \n",
              "7  selamat malam anda menyaksikan indonesia hari ...  \n",
              "8  dan saya adyanto vijaya selain dua topik utama...  "
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prediction_df = test_df.copy()\n",
        "normalizer = EnglishTextNormalizer()\n",
        "\n",
        "prediction_df['text_clean'] = prediction_df['text'].apply(normalizer)\n",
        "prediction_df['predict_clean'] = prediction_df['predict_zero_shot'].apply(normalizer)\n",
        "prediction_df[[\"text_clean\", \"predict_clean\"]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_MfAUjMsnVJ"
      },
      "source": [
        "#### Display Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCtgGgf0snVK",
        "outputId": "c963f3f1-ed03-4403-83a6-3cb748ff5155"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Evaluation Results ---\n",
            "    Model  WER (Original)  WER (Normalized)  CER (Original)  CER (Normalized)\n",
            "Zero-Shot        0.382114          0.169355        0.092233          0.046061\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n--- Evaluation Results ---\")\n",
        "# results_data = {\n",
        "#     'Model': ['Zero-Shot', 'From-Scratch', 'Fine-Tuned'],\n",
        "#     'WER (Original)': [wer_zero_shot_orig, wer_scratch_orig if 'wer_scratch_orig' in locals() else None, wer_fine_tuned_orig],\n",
        "#     'WER (Normalized)': [wer_zero_shot_norm, wer_scratch_norm if 'wer_scratch_norm' in locals() else None, wer_fine_tuned_norm],\n",
        "#     'CER (Original)': [cer_zero_shot_orig, cer_scratch_orig if 'cer_scratch_orig' in locals() else None, cer_fine_tuned_orig],\n",
        "#     'CER (Normalized)': [cer_zero_shot_norm, cer_scratch_norm if 'cer_scratch_norm' in locals() else None, cer_fine_tuned_norm]\n",
        "# }\n",
        "\n",
        "results_data = {\n",
        "    'Model': ['Zero-Shot'],\n",
        "    'WER (Original)': [wer_zero_shot_orig],\n",
        "    'WER (Normalized)': [wer_zero_shot_norm],\n",
        "    'CER (Original)': [cer_zero_shot_orig],\n",
        "    'CER (Normalized)': [cer_zero_shot_norm]\n",
        "}\n",
        "\n",
        "results_df = pd.DataFrame(results_data)\n",
        "print(results_df.to_string(index=False))\n",
        "\n",
        "# # Compare improvement relative to zero-shot baseline\n",
        "# print(\"\\n--- Improvement Relative to Zero-Shot ---\")\n",
        "# if 'wer_scratch_norm' in locals():\n",
        "#     wer_scratch_improvement = ((wer_zero_shot_norm - wer_scratch_norm) / wer_zero_shot_norm) * 100 if wer_zero_shot_norm != 0 else 0\n",
        "#     cer_scratch_improvement = ((cer_zero_shot_norm - cer_scratch_norm) / cer_zero_shot_norm) * 100 if cer_zero_shot_norm != 0 else 0\n",
        "# else:\n",
        "#     wer_scratch_improvement = None\n",
        "#     cer_scratch_improvement = None\n",
        "\n",
        "\n",
        "# wer_fine_tuned_improvement = ((wer_zero_shot_norm - wer_fine_tuned_norm) / wer_zero_shot_norm) * 100 if wer_zero_shot_norm != 0 else 0\n",
        "# cer_fine_tuned_improvement = ((cer_zero_shot_norm - cer_fine_tuned_norm) / cer_zero_shot_norm) * 100 if cer_zero_shot_norm != 0 else 0\n",
        "\n",
        "# improvement_data = {\n",
        "#     'Model': ['From-Scratch', 'Fine-Tuned'],\n",
        "#     'WER Improvement (%)': [wer_scratch_improvement, wer_fine_tuned_improvement],\n",
        "#     'CER Improvement (%)': [cer_scratch_improvement, cer_fine_tuned_improvement]\n",
        "# }\n",
        "# improvement_df = pd.DataFrame(improvement_data)\n",
        "# print(improvement_df.to_string(index=False))\n",
        "\n",
        "# fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "# fig.suptitle('ASR Model Performance Comparison', fontsize=16)\n",
        "\n",
        "# # WER Comparison\n",
        "# sns.barplot(x='Model', y='WER (Normalized)', data=results_df, ax=axes[0, 0])\n",
        "# axes[0, 0].set_title('Normalized WER Comparison')\n",
        "# axes[0, 0].set_ylabel('WER')\n",
        "\n",
        "# # CER Comparison\n",
        "# sns.barplot(x='Model', y='CER (Normalized)', data=results_df, ax=axes[0, 1])\n",
        "# axes[0, 1].set_title('Normalized CER Comparison')\n",
        "# axes[0, 1].set_ylabel('CER')\n",
        "\n",
        "# # Combined WER vs CER\n",
        "# results_melted = results_df.melt(id_vars='Model', value_vars=['WER (Normalized)', 'CER (Normalized)'], var_name='Metric', value_name='Rate')\n",
        "# sns.barplot(x='Model', y='Rate', hue='Metric', data=results_melted, ax=axes[1, 0])\n",
        "# axes[1, 0].set_title('Normalized WER vs CER Comparison')\n",
        "# axes[1, 0].set_ylabel('Rate')\n",
        "\n",
        "# # Step-based training curves (Loss vs Steps)\n",
        "# # Assuming train_losses_scratch and val_losses_scratch are available from previous execution\n",
        "# if 'train_losses_scratch' in locals() and 'val_losses_scratch' in locals():\n",
        "#     steps_scratch = range(1, len(train_losses_scratch) + 1)\n",
        "#     eval_steps_indices = [(i + 1) for i in range(len(train_losses_scratch)) if (i + 1) % 25 == 0]\n",
        "#     val_steps = [step for step in steps_scratch if step in eval_steps_indices]\n",
        "\n",
        "\n",
        "#     axes[1, 1].plot(steps_scratch, train_losses_scratch, label='Train Loss (Scratch)')\n",
        "#     if val_steps:\n",
        "#         axes[1, 1].plot(val_steps, val_losses_scratch, label='Validation Loss (Scratch)', marker='o')\n",
        "#     axes[1, 1].set_title('From-Scratch Training Loss vs Steps')\n",
        "#     axes[1, 1].set_xlabel('Steps')\n",
        "#     axes[1, 1].set_ylabel('Loss')\n",
        "#     axes[1, 1].legend()\n",
        "# else:\n",
        "#     axes[1, 1].set_title('From-Scratch Training Loss vs Steps (Data Not Available)')\n",
        "\n",
        "\n",
        "# plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "# plt.show()\n",
        "\n",
        "# # Save results to CSV\n",
        "# results_df.to_csv('asr_evaluation_results.csv', index=False)\n",
        "# if 'train_losses_scratch' in locals() and 'val_losses_scratch' in locals():\n",
        "#     training_progress_df = pd.DataFrame({\n",
        "#         'Step': steps_scratch,\n",
        "#         'Train Loss (Scratch)': train_losses_scratch,\n",
        "#     })\n",
        "#     if val_steps:\n",
        "#          training_progress_df['Validation Loss (Scratch)'] = pd.Series(val_losses_scratch, index=[i-1 for i in val_steps]) # Align val loss with steps\n",
        "#     training_progress_df.to_csv('scratch_training_progress.csv', index=False)\n",
        "\n",
        "\n",
        "# print(\"\\nEvaluation results saved to 'asr_evaluation_results.csv'\")\n",
        "# if 'train_losses_scratch' in locals() and 'val_losses_scratch' in locals():\n",
        "#     print(\"From-scratch training progress saved to 'scratch_training_progress.csv'\")\n",
        "\n",
        "# print(\"\\n--- Experiment Summary ---\")\n",
        "# print(f\"Dataset size: {len(audio_transcript_pair_list)} samples\")\n",
        "# print(f\"Train samples: {len(train_list)}\")\n",
        "# print(f\"Val samples: {len(val_list)}\")\n",
        "# print(f\"Test samples: {len(test_list)}\")\n",
        "# print(f\"Max training steps: {cfg.max_steps}\")\n",
        "# print(f\"Eval every: {cfg.eval_steps} steps\")\n",
        "# print(f\"Fine-tuned model checkpoint saved to: artifacts/checkpoint\")\n",
        "# print(f\"From-scratch model checkpoint saved to: {'best_scratch_model_indonesia.pth' if os.path.exists('best_scratch_model_indonesia.pth') else 'N/A'}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1056da3cdb5e4d00aeed254520d60f9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1833f9a3e150442eb362d962c0836fd7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2a4447d1e4a74589888e84971abc6a63": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c9f93ea4cb754322984790791ccdd846",
            "placeholder": "",
            "style": "IPY_MODEL_1833f9a3e150442eb362d962c0836fd7",
            "value": "8/8[00:00&lt;00:00,618.93it/s]"
          }
        },
        "333449d944cf4c489577b79c9c3f1eb4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "37f7f201af7c4da2a192cc33270c8b41": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7196a5b5857e4c928d1efcffd259350c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8e8e055ebd45452789c7b87a5d25fb1c",
              "IPY_MODEL_c0a5ae1b303f4b1aa3d6838397284622",
              "IPY_MODEL_e9c6b87fc5dd4b669dccb47207ea5855"
            ],
            "layout": "IPY_MODEL_e2b5bb9426a542478f22c13053a3ea9a"
          }
        },
        "7973d56aaeee4a8f955527b865b3ec15": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "809ba883b2f3498ba012b12dcf866192": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_92bc7ae0c4eb4aff997002401e970e1c",
              "IPY_MODEL_bc1e947df296454fa616cd3a60f6f407",
              "IPY_MODEL_2a4447d1e4a74589888e84971abc6a63"
            ],
            "layout": "IPY_MODEL_c71060239d844e649a054c9b28f43883"
          }
        },
        "88cfe2d442d1425c9ce0af3bd0b15c37": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8e8e055ebd45452789c7b87a5d25fb1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_37f7f201af7c4da2a192cc33270c8b41",
            "placeholder": "",
            "style": "IPY_MODEL_88cfe2d442d1425c9ce0af3bd0b15c37",
            "value": "TranscribingZero-Shot:100%"
          }
        },
        "92bc7ae0c4eb4aff997002401e970e1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7973d56aaeee4a8f955527b865b3ec15",
            "placeholder": "",
            "style": "IPY_MODEL_1056da3cdb5e4d00aeed254520d60f9d",
            "value": "TranscribingFine-Tuned:100%"
          }
        },
        "a966a961f12c45359f7589dedc761855": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ba1f48cd5fc349f08b983a1b96c300fe": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb7b3593c9d141a1815bb277b074d54c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bc1e947df296454fa616cd3a60f6f407": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_333449d944cf4c489577b79c9c3f1eb4",
            "max": 8,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bb7b3593c9d141a1815bb277b074d54c",
            "value": 8
          }
        },
        "c0a5ae1b303f4b1aa3d6838397284622": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dc81dd135d8f4d00bc0ca2375180abc8",
            "max": 9,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dea60e6019c445d6bc1b5a0a9cbf26bf",
            "value": 9
          }
        },
        "c71060239d844e649a054c9b28f43883": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c9f93ea4cb754322984790791ccdd846": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc81dd135d8f4d00bc0ca2375180abc8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dea60e6019c445d6bc1b5a0a9cbf26bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e2b5bb9426a542478f22c13053a3ea9a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e9c6b87fc5dd4b669dccb47207ea5855": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ba1f48cd5fc349f08b983a1b96c300fe",
            "placeholder": "",
            "style": "IPY_MODEL_a966a961f12c45359f7589dedc761855",
            "value": "9/9[02:07&lt;00:00,15.01s/it]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
